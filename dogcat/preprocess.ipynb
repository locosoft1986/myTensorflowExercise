{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "from skimage import img_as_ubyte\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATASET_PATH = \"datasets\"\n",
    "DATASET_TRAIN_PATH = \"train\"\n",
    "DATASET_TEST_PATH = \"test\"\n",
    "DATASET_OUTPUT_PATH = \"outputs\"\n",
    "\n",
    "DEVSET_TOTAL_RATIO = 0.1\n",
    "\n",
    "NUM_THREADS = 8\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "TRAINSET_SOURCE_PATH =  \"%s/%s/%s\" % (os.getcwd(), DATASET_PATH, DATASET_TRAIN_PATH)\n",
    "TESTSET_SOURCE_PATH = \"%s/%s/%s\" % (os.getcwd(), DATASET_PATH, DATASET_TEST_PATH)\n",
    "\n",
    "train_image_names = [i for i in os.listdir(TRAINSET_SOURCE_PATH)]\n",
    "train_image_labels = np.array([(1 if 'dog' in i else 0) for i in train_image_names]).astype(np.int64)\n",
    "\n",
    "DEVSET_TOTAL = int(len(train_image_names) * 0.1)\n",
    "dev_images_idx = np.random.choice(len(train_image_names), DEVSET_TOTAL, replace=False)\n",
    "\n",
    "full_train_image_path = [\"%s/%s\" % (TRAINSET_SOURCE_PATH, i) for i in train_image_names]\n",
    "\n",
    "dev_images = [full_train_image_path[i] for i in dev_images_idx]\n",
    "dev_labels = train_image_labels[dev_images_idx]\n",
    "\n",
    "train_images = [elem for i, elem in enumerate(full_train_image_path) if i not in dev_images_idx]\n",
    "train_labels = np.delete(train_image_labels, dev_images_idx)\n",
    "\n",
    "test_images = [\"%s/%s\" % (TESTSET_SOURCE_PATH, i) for i in os.listdir(TESTSET_SOURCE_PATH)]\n",
    "test_labels = (np.ones((len(test_images))) * -1).astype(np.int64)\n",
    "\n",
    "print('Total Image Size: ', len(train_image_names))\n",
    "print('Training Set Images Size: ', len(train_images))\n",
    "print('Training Set Labels Shape: ', train_labels.shape)\n",
    "print('Dev Set Images Size: ', len(dev_images))\n",
    "print('Dev Set Labels Shape: ', dev_labels.shape)\n",
    "print('Test Set Images Size:', len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def resize_padded(img, size):   \n",
    "    from skimage.transform import resize\n",
    "    \n",
    "    width = img.shape[0]\n",
    "    height = img.shape[1]\n",
    "    channel = img.shape[2]\n",
    "    new_width = width\n",
    "    new_height = height\n",
    "    x = 0\n",
    "    y = 0\n",
    "    if height > width:\n",
    "        new_height = size\n",
    "        new_width = int((width/height) * size)\n",
    "        y = 0\n",
    "        x = int(( size - new_width ) / 2)\n",
    "    else:        \n",
    "        new_width = size\n",
    "        new_height = int((height/width) * size)\n",
    "        x = 0\n",
    "        y = int(( size - new_height ) / 2)\n",
    "    \n",
    "    interm_shape = (new_width, new_height)\n",
    "    new_shape = (size, size, channel)\n",
    "    \n",
    "    interm_img = resize(img, interm_shape)\n",
    "    \n",
    "    new_img = np.empty((size, size, channel), dtype=interm_img.dtype)\n",
    "    new_img.fill(0)\n",
    "    \n",
    "    new_img[x:new_width+x, y:new_height+y] = interm_img\n",
    "    \n",
    "    return img_as_ubyte(new_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_examples_paths = [train_images[i] for i in np.random.choice(len(train_images), 3, replace=False)]\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "for i, file_path in enumerate(image_examples_paths):\n",
    "    plt.subplot(2,3,i + 1)\n",
    "    image = io.imread(file_path)\n",
    "    img = plt.imshow(image, interpolation='nearest')\n",
    "    img.axes.get_xaxis().set_visible(False)\n",
    "    img.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    image_resized = resize_padded(image, IMAGE_SIZE)\n",
    "    plt.subplot(2,3,i + 4)\n",
    "    img = plt.imshow(image_resized, interpolation='nearest')\n",
    "    img.axes.get_xaxis().set_visible(False)\n",
    "    img.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _convert_to_example(filename, image_buffer, label, width, height):\n",
    "    \"\"\"Build an Example proto for an example.\n",
    "    Args:\n",
    "    filename: string, path to an image file, e.g., '/path/to/example.JPG'\n",
    "    image_buffer: string, JPEG encoding of RGB image\n",
    "    label: integer, identifier for the ground truth for the network\n",
    "    height: integer, image height in pixels\n",
    "    width: integer, image width in pixels\n",
    "    Returns:\n",
    "    Example proto\n",
    "    \"\"\"\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'width': _int64_feature(width),\n",
    "        'height': _int64_feature(height),\n",
    "        'label': _int64_feature(label),\n",
    "        'filename': _bytes_feature(tf.compat.as_bytes(os.path.basename(filename))),\n",
    "        'image': _bytes_feature(tf.compat.as_bytes((image_buffer)))\n",
    "    }))\n",
    "    return example\n",
    "\n",
    "\n",
    "def _process_image(filename):\n",
    "    \"\"\"Process a single image file.\n",
    "    Args:\n",
    "      filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
    "    Returns:\n",
    "      image_buffer: string, JPEG encoding of RGB image.\n",
    "      height: integer, image height in pixels.\n",
    "      width: integer, image width in pixels.\n",
    "    \"\"\"\n",
    "    image = resize_padded(io.imread(filename), IMAGE_SIZE)\n",
    "\n",
    "    # Check that image converted to RGB\n",
    "    assert len(image.shape) == 3\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    assert image.shape[2] == 3\n",
    "\n",
    "    return image.tobytes(), height, width\n",
    "\n",
    "def _process_image_files_batch(thread_index, ranges, name, filenames,\n",
    "                               labels, num_shards):\n",
    "    \"\"\"Processes and saves list of images as TFRecord in 1 thread.\n",
    "    Args:\n",
    "      thread_index: integer, unique batch to run index is within [0, len(ranges)).\n",
    "      ranges: list of pairs of integers specifying ranges of each batches to\n",
    "        analyze in parallel.\n",
    "      name: string, unique identifier specifying the data set\n",
    "      filenames: list of strings; each string is a path to an image file\n",
    "      labels: list of integer; each integer identifies the ground truth\n",
    "      num_shards: integer number of shards for this data set.\n",
    "    \"\"\"\n",
    "    # Each thread produces N shards where N = int(num_shards / num_threads).\n",
    "    # For instance, if num_shards = 128, and the num_threads = 2, then the first\n",
    "    # thread would produce shards [0, 64).\n",
    "    num_threads = len(ranges)\n",
    "    assert not num_shards % num_threads\n",
    "    num_shards_per_batch = int(num_shards / num_threads)\n",
    "\n",
    "    shard_ranges = np.linspace(ranges[thread_index][0],\n",
    "                               ranges[thread_index][1],\n",
    "                               num_shards_per_batch + 1).astype(int)\n",
    "    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "\n",
    "    counter = 0\n",
    "    for s in range(num_shards_per_batch):\n",
    "        # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'\n",
    "        shard = thread_index * num_shards_per_batch + s\n",
    "        output_filename = '%s-%.5d-of-%.5d.tfrecord' % (name, shard, num_shards)\n",
    "        output_file = os.path.join(DATASET_OUTPUT_PATH, output_filename)\n",
    "        writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "        shard_counter = 0\n",
    "        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n",
    "        for i in files_in_shard:\n",
    "            filename = filenames[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            image_buffer, height, width = _process_image(filename)\n",
    "\n",
    "            example = _convert_to_example(filename, image_buffer, label, width, height)\n",
    "            writer.write(example.SerializeToString())\n",
    "            shard_counter += 1\n",
    "            counter += 1\n",
    "\n",
    "            if not counter % 1000:\n",
    "                print('%s [thread %d]: Processed %d of %d images in thread batch.' %\n",
    "                      (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        writer.close()\n",
    "        print('%s [thread %d]: Wrote %d images to %s' %\n",
    "              (datetime.now(), thread_index, shard_counter, output_file))\n",
    "        sys.stdout.flush()\n",
    "        shard_counter = 0\n",
    "        print('%s [thread %d]: Wrote %d images to %d shards.' %\n",
    "              (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "def process_image_files(name, filenames, labels, num_shards):\n",
    "    \"\"\"Process and save list of images as TFRecord of Example protos.\n",
    "    Args:\n",
    "    name: string, unique identifier specifying the data set\n",
    "    filenames: list of strings; each string is a path to an image file\n",
    "    labels: list of integer; each integer identifies the ground truth\n",
    "    num_shards: integer number of shards for this data set.\n",
    "    \"\"\"\n",
    "    assert len(filenames) == len(labels)\n",
    "\n",
    "    # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n",
    "    spacing = np.linspace(0, len(filenames), NUM_THREADS + 1).astype(np.int)\n",
    "    ranges = []\n",
    "    for i in range(len(spacing) - 1):\n",
    "        ranges.append([spacing[i], spacing[i + 1]])\n",
    "\n",
    "    # Launch a thread for each batch.\n",
    "    print('Launching %d threads for spacings: %s' % (NUM_THREADS, ranges))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Create a mechanism for monitoring when all threads are finished.\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "\n",
    "    threads = []\n",
    "    for thread_index in range(len(ranges)):\n",
    "        args = (thread_index, ranges, name, filenames,\n",
    "                labels, num_shards)\n",
    "        t = threading.Thread(target=_process_image_files_batch, args=args)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # Wait for all the threads to terminate.\n",
    "    coord.join(threads)\n",
    "    print('%s: Finished writing all %d images in data set.' %\n",
    "          (datetime.now(), len(filenames)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_image_files('dev', dev_images, dev_labels, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_image_files('test', test_images, test_labels, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_image_files('train', train_images, train_labels, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
